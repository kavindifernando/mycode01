{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e1de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from random import randint, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a611dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_features(selected_features, all_features):\n",
    "    selected_f = []\n",
    "    for sample in all_features:\n",
    "        s_f = []\n",
    "        for i in range(len(selected_features)):\n",
    "            if selected_features[i] == 1:\n",
    "                s_f.append(sample[i])\n",
    "        selected_f.append(s_f)\n",
    "    return selected_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "773e6e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_labels(class_prob):\n",
    "    predicted_labels = []\n",
    "    \n",
    "    for p in class_prob:\n",
    "        predicted_labels.append(p.argmax() + 1)\n",
    "            \n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08b1be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_miss_percentages(true_labels, predicted_labels):\n",
    "    c1_missed = 0\n",
    "    c1_tot = 0\n",
    "    c2_missed = 0\n",
    "    c2_tot = 0\n",
    "    c3_missed = 0\n",
    "    c3_tot = 0\n",
    "    \n",
    "    for i in range(len(true_labels)):\n",
    "        if true_labels[i] == 1:\n",
    "            c1_tot += 1\n",
    "            if true_labels[i] != predicted_labels[i]:\n",
    "                c1_missed += 1\n",
    "        \n",
    "        if true_labels[i] == 2:\n",
    "            c2_tot += 1\n",
    "            if true_labels[i] != predicted_labels[i]:\n",
    "                c2_missed += 1\n",
    "                \n",
    "        if true_labels[i] == 3:\n",
    "            c3_tot += 1\n",
    "            if true_labels[i] != predicted_labels[i]:\n",
    "                c3_missed += 1\n",
    "        \n",
    "    c1_miss_percent = (100.00 * c1_missed) / c1_tot  \n",
    "    c2_miss_percent = (100.00 * c2_missed) / c2_tot\n",
    "    c3_miss_percent = (100.00 * c3_missed) / c3_tot\n",
    "    \n",
    "    if c1_miss_percent <= 1:\n",
    "        c1_miss_percent = 1\n",
    "    if c2_miss_percent <= 1:\n",
    "        c2_miss_percent = 1\n",
    "    if c3_miss_percent <= 1:\n",
    "        c3_miss_percent = 1\n",
    "        \n",
    "    print( \"Missed samples for each class: \", c1_missed, c2_missed, c3_missed)\n",
    "    \n",
    "    return c1_miss_percent, c2_miss_percent, c3_miss_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8656d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train = open(\"ann-train.data\", \"r\")\n",
    "    training_data = []\n",
    "    for line in train:\n",
    "        training_data.append(line.strip().split(\" \"))\n",
    "    \n",
    "    convert_types(training_data)\n",
    "    \n",
    "    test = open(\"ann-test.data\", \"r\")\n",
    "    test_data = []\n",
    "    for line in test:\n",
    "        test_data.append(line.strip().split(\" \"))\n",
    "    \n",
    "    convert_types(test_data)\n",
    "    \n",
    "    training_features = []\n",
    "    training_labels = []\n",
    "    for sample in training_data:\n",
    "        training_features.append(sample[:-1])\n",
    "        training_labels.append(sample[-1])\n",
    "        \n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "    for sample in test_data:\n",
    "        test_features.append(sample[:-1])\n",
    "        test_labels.append(sample[-1])\n",
    "        \n",
    "    cost = open(\"ann-thyroid.cost\", \"r\")\n",
    "    costs = []\n",
    "    for line in cost:\n",
    "        costs.append(float(line.strip().split(\":\")[1]))\n",
    "    costs.append(0) # 21st feature is a comb. of 19th and 20th features\n",
    "        \n",
    "    return training_features, training_labels, test_features, test_labels, costs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "749bf519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_types(data):\n",
    "    for row in data:\n",
    "        row[0] = float(row[0].strip())\n",
    "        row[1] = int(row[1].strip())\n",
    "        row[2] = int(row[2].strip())\n",
    "        row[3] = int(row[3].strip())\n",
    "        row[4] = int(row[4].strip())\n",
    "        row[5] = int(row[5].strip())\n",
    "        row[6] = int(row[6].strip())\n",
    "        row[7] = int(row[7].strip())\n",
    "        row[8] = int(row[8].strip())\n",
    "        row[9] = int(row[9].strip())\n",
    "        row[10] = int(row[10].strip())\n",
    "        row[11] = int(row[11].strip())\n",
    "        row[12] = int(row[12].strip())\n",
    "        row[13] = int(row[13].strip())\n",
    "        row[14] = int(row[14].strip())\n",
    "        row[15] = int(row[15].strip())\n",
    "        row[16] = float(row[16].strip())\n",
    "        row[17] = float(row[17].strip())\n",
    "        row[18] = float(row[18].strip())\n",
    "        row[19] = float(row[19].strip())\n",
    "        row[20] = float(row[20].strip())\n",
    "        row[21] = int(row[21].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d434533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual(length):\n",
    "    # create an individual, which is binary repr. of selected features\n",
    "    selected_features = [0] * len(length)\n",
    "    \n",
    "    for i in range(len(selected_features)):\n",
    "        selected_features[i] = randint(0, 1)\n",
    "        \n",
    "    # 21st feature is a comb. of 19th and 20th features\n",
    "    if selected_features[20] == 1: \n",
    "        selected_features[18] = 1\n",
    "        selected_features[19] = 1\n",
    "        \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "511b317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def population(count, length):\n",
    "    # create 'count' number of individuals\n",
    "    return [individual(length) for _ in range(count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "401d1efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(individual, clf, data):\n",
    "    # determine the fitness of an individual\n",
    "    # misclassified class 1 (%) * misclassified class 2 (%) * misclassified class 3 (%) * feature selection cost (resulting value is converted to int)\n",
    "    # lower is better\n",
    "    print (\"\\n######################################################################\")\n",
    "    print (\"Individual: \", individual)\n",
    "    fs_cost = feature_selection_cost(individual, data[4])\n",
    "    \n",
    "    selected_train_f = get_selected_features(individual, data[0])\n",
    "    selected_test_f = get_selected_features(individual, data[2])\n",
    "\n",
    "    clf = clf.fit(selected_train_f, data[1])\n",
    "    \n",
    "    class_prob = clf.predict_proba(selected_test_f)\n",
    "    predicted_labels = get_predicted_labels(class_prob)\n",
    "    target_names = ['class 1', 'class 2', 'class 3']\n",
    "    \n",
    "    print(classification_report(data[3], predicted_labels, target_names=target_names))\n",
    "    print( \"Mean accuracy: \", clf.score(selected_test_f, data[3]))\n",
    "    print (\"No of correctly classified samples: \", accuracy_score(data[3], predicted_labels, normalize=False))\n",
    "    \n",
    "    c1_miss_percent, c2_miss_percent, c3_miss_percent = get_class_miss_percentages(data[3], predicted_labels)\n",
    "    f_result = int(c1_miss_percent * c2_miss_percent * c3_miss_percent * fs_cost)\n",
    "    \n",
    "    print (\"\\nClass accuracies: \\n\", \"class 1: \", (100 - c1_miss_percent), \"%\\nclass 2: \", (100 - c2_miss_percent), \"%\\nclass 3: \", (100 - c3_miss_percent), \"%\\n\")\n",
    "    print (\"Feature selection cost: \", fs_cost)\n",
    "    print (\"Fitness: \", f_result)\n",
    "    print (\"######################################################################\\n\")\n",
    "    \n",
    "    return f_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ade1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_fitness(pop, clf, data):\n",
    "    # average fitness of a population\n",
    "    tot_fitness = 0\n",
    "    for i in pop:\n",
    "        tot_fitness += fitness(i, clf, data)\n",
    "    return tot_fitness / len(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b44b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve(pop, clf, data, retain_percentage=0.50, random_select=0.05, mutate_prob=0.01):\n",
    "    f_values = [(fitness(i, clf, data), i) for i in pop]\n",
    "    individuals = [i[1] for i in sorted(f_values)]\n",
    "    retain_length = int(len(pop) * retain_percentage)\n",
    "    parents = individuals[:retain_length]\n",
    "    \n",
    "    # randomly add other individuals to increase diversity\n",
    "    for i in individuals[retain_length:]:\n",
    "        if random_select > random():\n",
    "            parents.append(i)\n",
    "            \n",
    "    # mutate\n",
    "    for i in parents:\n",
    "        if mutate_prob > random():\n",
    "            index_to_mutate = randint(0, len(i) - 1)\n",
    "            i[index_to_mutate] = randint(0, 1)\n",
    "            \n",
    "            # make sure the result is still valid\n",
    "            if i[20] == 1: \n",
    "                i[18] = 1\n",
    "                i[19] = 1\n",
    "    \n",
    "    # crossover\n",
    "    no_of_parents = len(parents)\n",
    "    remaining_no_of_ind = len(pop) - no_of_parents\n",
    "    children = []\n",
    "    \n",
    "    while len(children) < remaining_no_of_ind:\n",
    "        male_index = randint(0, no_of_parents - 1)\n",
    "        female_index = randint(0, no_of_parents - 1)\n",
    "        \n",
    "        if male_index != female_index:\n",
    "            male = parents[male_index]\n",
    "            female = parents[female_index]\n",
    "            half = len(male) / 2\n",
    "            child = male[:half] + female[half:]\n",
    "            children.append(child)\n",
    "    \n",
    "    parents.extend(children)\n",
    "    return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dac7c16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_cost(selected_features, costs):\n",
    "    total_cost = 0\n",
    "    \n",
    "    for i in range(len(selected_features)):\n",
    "        if selected_features[i] == 1:\n",
    "            total_cost += costs[i]\n",
    "            \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccfe69e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fittest_results(clf, training_features, training_labels, test_features, test_labels, costs):\n",
    "    # obtained from genetic algorithm runs\n",
    "    fittest = [0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1]\n",
    "    \n",
    "    print (\"Test accuracies\")\n",
    "    test_acc_data = [training_features, training_labels, test_features, test_labels, costs]\n",
    "    fitness(fittest, clf, test_acc_data)\n",
    "    \n",
    "    print (\"Training accuracies\")\n",
    "    train_acc_data = [training_features, training_labels, training_features, training_labels, costs]\n",
    "    fitness(fittest, clf, train_acc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a0da4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    training_features, training_labels, test_features, test_labels, costs = load_data()\n",
    "    data = [training_features, training_labels, test_features, test_labels, costs]\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    \n",
    "    pop = population(20, training_features[0])\n",
    "    \n",
    "    fitness_history = []\n",
    "    for i in range(100):\n",
    "        pop = evolve(pop, clf, data)\n",
    "        pop_fitness = avg_fitness(pop, clf, data)\n",
    "        fitness_history.append(pop_fitness)\n",
    "        \n",
    "        if pop_fitness < 250:\n",
    "            break\n",
    "\n",
    "    print (\"Avg fitness history: \", fitness_history)\n",
    "    \n",
    "    # fittest_results(clf, training_features, training_labels, test_features, test_labels, costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfbf8a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "Individual:  [0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.66      0.81      0.73        73\n",
      "     class 2       0.85      0.87      0.86       177\n",
      "     class 3       1.00      0.99      0.99      3178\n",
      "\n",
      "    accuracy                           0.98      3428\n",
      "   macro avg       0.84      0.89      0.86      3428\n",
      "weighted avg       0.98      0.98      0.98      3428\n",
      "\n",
      "Mean accuracy:  0.9801633605600933\n",
      "No of correctly classified samples:  3360\n",
      "Missed samples for each class:  14 23 31\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  80.82191780821918 %\n",
      "class 2:  87.00564971751413 %\n",
      "class 3:  99 %\n",
      "\n",
      "Feature selection cost:  45.19\n",
      "Fitness:  11261\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.91      0.95      0.93        73\n",
      "     class 2       0.94      1.00      0.97       177\n",
      "     class 3       1.00      0.99      1.00      3178\n",
      "\n",
      "    accuracy                           0.99      3428\n",
      "   macro avg       0.95      0.98      0.96      3428\n",
      "weighted avg       0.99      0.99      0.99      3428\n",
      "\n",
      "Mean accuracy:  0.9935822637106184\n",
      "No of correctly classified samples:  3406\n",
      "Missed samples for each class:  4 0 18\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  94.52054794520548 %\n",
      "class 2:  99 %\n",
      "class 3:  99 %\n",
      "\n",
      "Feature selection cost:  67.11\n",
      "Fitness:  367\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.44      0.74      0.55        73\n",
      "     class 2       0.10      0.11      0.11       177\n",
      "     class 3       0.94      0.93      0.94      3178\n",
      "\n",
      "    accuracy                           0.88      3428\n",
      "   macro avg       0.50      0.59      0.53      3428\n",
      "weighted avg       0.89      0.88      0.89      3428\n",
      "\n",
      "Mean accuracy:  0.8815635939323221\n",
      "No of correctly classified samples:  3022\n",
      "Missed samples for each class:  19 158 229\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  73.97260273972603 %\n",
      "class 2:  10.734463276836152 %\n",
      "class 3:  92.79421019509125 %\n",
      "\n",
      "Feature selection cost:  32.92\n",
      "Fitness:  551132\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.86      0.96      0.91        73\n",
      "     class 2       0.94      1.00      0.97       177\n",
      "     class 3       1.00      0.99      1.00      3178\n",
      "\n",
      "    accuracy                           0.99      3428\n",
      "   macro avg       0.93      0.98      0.96      3428\n",
      "weighted avg       0.99      0.99      0.99      3428\n",
      "\n",
      "Mean accuracy:  0.9927071178529755\n",
      "No of correctly classified samples:  3403\n",
      "Missed samples for each class:  3 0 22\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  95.89041095890411 %\n",
      "class 2:  99 %\n",
      "class 3:  99 %\n",
      "\n",
      "Feature selection cost:  57.7\n",
      "Fitness:  237\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.43      0.53      0.48        73\n",
      "     class 2       0.57      0.59      0.58       177\n",
      "     class 3       0.98      0.98      0.98      3178\n",
      "\n",
      "    accuracy                           0.95      3428\n",
      "   macro avg       0.66      0.70      0.68      3428\n",
      "weighted avg       0.95      0.95      0.95      3428\n",
      "\n",
      "Mean accuracy:  0.9474912485414235\n",
      "No of correctly classified samples:  3248\n",
      "Missed samples for each class:  34 72 74\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  53.42465753424658 %\n",
      "class 2:  59.32203389830509 %\n",
      "class 3:  97.67149150409062 %\n",
      "\n",
      "Feature selection cost:  30.78\n",
      "Fitness:  135788\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.41      0.63      0.50        73\n",
      "     class 2       0.08      0.09      0.09       177\n",
      "     class 3       0.94      0.93      0.93      3178\n",
      "\n",
      "    accuracy                           0.88      3428\n",
      "   macro avg       0.48      0.55      0.51      3428\n",
      "weighted avg       0.89      0.88      0.88      3428\n",
      "\n",
      "Mean accuracy:  0.8766044340723453\n",
      "No of correctly classified samples:  3005\n",
      "Missed samples for each class:  27 161 235\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  63.013698630136986 %\n",
      "class 2:  9.039548022598865 %\n",
      "class 3:  92.60541220893644 %\n",
      "\n",
      "Feature selection cost:  26.509999999999998\n",
      "Fitness:  659503\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.57      0.68      0.62        73\n",
      "     class 2       0.15      0.15      0.15       177\n",
      "     class 3       0.95      0.94      0.94      3178\n",
      "\n",
      "    accuracy                           0.89      3428\n",
      "   macro avg       0.56      0.59      0.57      3428\n",
      "weighted avg       0.90      0.89      0.90      3428\n",
      "\n",
      "Mean accuracy:  0.8949824970828472\n",
      "No of correctly classified samples:  3068\n",
      "Missed samples for each class:  23 150 187\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  68.4931506849315 %\n",
      "class 2:  15.254237288135599 %\n",
      "class 3:  94.11579609817495 %\n",
      "\n",
      "Feature selection cost:  33.92\n",
      "Fitness:  532925\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.83      0.95      0.88        73\n",
      "     class 2       0.74      0.82      0.78       177\n",
      "     class 3       0.99      0.98      0.99      3178\n",
      "\n",
      "    accuracy                           0.97      3428\n",
      "   macro avg       0.85      0.91      0.88      3428\n",
      "weighted avg       0.97      0.97      0.97      3428\n",
      "\n",
      "Mean accuracy:  0.9714119019836639\n",
      "No of correctly classified samples:  3330\n",
      "Missed samples for each class:  4 32 62\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  94.52054794520548 %\n",
      "class 2:  81.92090395480226 %\n",
      "class 3:  98.04908747640025 %\n",
      "\n",
      "Feature selection cost:  66.11\n",
      "Fitness:  12776\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.00      0.00      0.00        73\n",
      "     class 2       0.07      0.02      0.03       177\n",
      "     class 3       0.93      0.98      0.95      3178\n",
      "\n",
      "    accuracy                           0.91      3428\n",
      "   macro avg       0.33      0.33      0.33      3428\n",
      "weighted avg       0.86      0.91      0.89      3428\n",
      "\n",
      "Mean accuracy:  0.912485414235706\n",
      "No of correctly classified samples:  3128\n",
      "Missed samples for each class:  73 174 53\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  0.0 %\n",
      "class 2:  1.6949152542372872 %\n",
      "class 3:  98.33228445563248 %\n",
      "\n",
      "Feature selection cost:  7.0\n",
      "Fitness:  114761\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.48      0.62      0.54        73\n",
      "     class 2       0.04      0.01      0.02       177\n",
      "     class 3       0.94      0.97      0.95      3178\n",
      "\n",
      "    accuracy                           0.91      3428\n",
      "   macro avg       0.49      0.53      0.50      3428\n",
      "weighted avg       0.88      0.91      0.90      3428\n",
      "\n",
      "Mean accuracy:  0.9121936989498249\n",
      "No of correctly classified samples:  3127\n",
      "Missed samples for each class:  28 175 98\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  61.64383561643836 %\n",
      "class 2:  1.1299435028248581 %\n",
      "class 3:  96.91629955947137 %\n",
      "\n",
      "Feature selection cost:  21.509999999999998\n",
      "Fitness:  251543\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.06      0.03      0.04        73\n",
      "     class 2       0.12      0.05      0.07       177\n",
      "     class 3       0.93      0.97      0.95      3178\n",
      "\n",
      "    accuracy                           0.90      3428\n",
      "   macro avg       0.37      0.35      0.35      3428\n",
      "weighted avg       0.87      0.90      0.88      3428\n",
      "\n",
      "Mean accuracy:  0.9016919486581096\n",
      "No of correctly classified samples:  3091\n",
      "Missed samples for each class:  71 168 98\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  2.7397260273972535 %\n",
      "class 2:  5.0847457627118615 %\n",
      "class 3:  96.91629955947137 %\n",
      "\n",
      "Feature selection cost:  22.41\n",
      "Fitness:  637948\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.54      0.70      0.61        73\n",
      "     class 2       0.10      0.10      0.10       177\n",
      "     class 3       0.94      0.93      0.94      3178\n",
      "\n",
      "    accuracy                           0.89      3428\n",
      "   macro avg       0.53      0.58      0.55      3428\n",
      "weighted avg       0.89      0.89      0.89      3428\n",
      "\n",
      "Mean accuracy:  0.8850641773628938\n",
      "No of correctly classified samples:  3034\n",
      "Missed samples for each class:  22 159 213\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  69.86301369863014 %\n",
      "class 2:  10.169491525423723 %\n",
      "class 3:  93.29767149150409 %\n",
      "\n",
      "Feature selection cost:  30.919999999999998\n",
      "Fitness:  561033\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.90      0.88      0.89        73\n",
      "     class 2       0.94      0.99      0.96       177\n",
      "     class 3       1.00      0.99      1.00      3178\n",
      "\n",
      "    accuracy                           0.99      3428\n",
      "   macro avg       0.95      0.95      0.95      3428\n",
      "weighted avg       0.99      0.99      0.99      3428\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:  0.9915402567094516\n",
      "No of correctly classified samples:  3399\n",
      "Missed samples for each class:  9 2 18\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  87.67123287671232 %\n",
      "class 2:  98.87005649717514 %\n",
      "class 3:  99 %\n",
      "\n",
      "Feature selection cost:  69.11\n",
      "Fitness:  962\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.60      0.79      0.68        73\n",
      "     class 2       0.72      0.72      0.72       177\n",
      "     class 3       0.99      0.98      0.98      3178\n",
      "\n",
      "    accuracy                           0.96      3428\n",
      "   macro avg       0.77      0.83      0.80      3428\n",
      "weighted avg       0.97      0.96      0.96      3428\n",
      "\n",
      "Mean accuracy:  0.9629521586931156\n",
      "No of correctly classified samples:  3301\n",
      "Missed samples for each class:  15 49 63\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  79.45205479452055 %\n",
      "class 2:  72.31638418079096 %\n",
      "class 3:  98.01762114537445 %\n",
      "\n",
      "Feature selection cost:  42.19\n",
      "Fitness:  47575\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.88      0.97      0.92        73\n",
      "     class 2       0.74      0.71      0.73       177\n",
      "     class 3       0.98      0.98      0.98      3178\n",
      "\n",
      "    accuracy                           0.97      3428\n",
      "   macro avg       0.87      0.89      0.88      3428\n",
      "weighted avg       0.97      0.97      0.97      3428\n",
      "\n",
      "Mean accuracy:  0.9687864644107351\n",
      "No of correctly classified samples:  3321\n",
      "Missed samples for each class:  2 51 54\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  97.26027397260275 %\n",
      "class 2:  71.1864406779661 %\n",
      "class 3:  98.30081812460666 %\n",
      "\n",
      "Feature selection cost:  57.7\n",
      "Fitness:  7739\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.54      0.59      0.57        73\n",
      "     class 2       0.74      0.77      0.75       177\n",
      "     class 3       0.99      0.99      0.99      3178\n",
      "\n",
      "    accuracy                           0.97      3428\n",
      "   macro avg       0.76      0.78      0.77      3428\n",
      "weighted avg       0.97      0.97      0.97      3428\n",
      "\n",
      "Mean accuracy:  0.9673278879813302\n",
      "No of correctly classified samples:  3316\n",
      "Missed samples for each class:  30 41 41\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  58.9041095890411 %\n",
      "class 2:  76.8361581920904 %\n",
      "class 3:  98.7098804279421 %\n",
      "\n",
      "Feature selection cost:  30.78\n",
      "Fitness:  37801\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.68      0.74      0.71        73\n",
      "     class 2       0.65      0.77      0.71       177\n",
      "     class 3       0.99      0.98      0.98      3178\n",
      "\n",
      "    accuracy                           0.96      3428\n",
      "   macro avg       0.77      0.83      0.80      3428\n",
      "weighted avg       0.96      0.96      0.96      3428\n",
      "\n",
      "Mean accuracy:  0.9606184364060677\n",
      "No of correctly classified samples:  3293\n",
      "Missed samples for each class:  19 41 75\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  73.97260273972603 %\n",
      "class 2:  76.8361581920904 %\n",
      "class 3:  97.64002517306483 %\n",
      "\n",
      "Feature selection cost:  53.599999999999994\n",
      "Fitness:  76262\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.86      0.90      0.88        73\n",
      "     class 2       0.72      0.67      0.69       177\n",
      "     class 3       0.98      0.98      0.98      3178\n",
      "\n",
      "    accuracy                           0.96      3428\n",
      "   macro avg       0.85      0.85      0.85      3428\n",
      "weighted avg       0.96      0.96      0.96      3428\n",
      "\n",
      "Mean accuracy:  0.9641190198366394\n",
      "No of correctly classified samples:  3305\n",
      "Missed samples for each class:  7 59 57\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  90.41095890410959 %\n",
      "class 2:  66.66666666666666 %\n",
      "class 3:  98.20641913152926 %\n",
      "\n",
      "Feature selection cost:  55.7\n",
      "Fitness:  31932\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.90      0.88      0.89        73\n",
      "     class 2       0.77      0.86      0.82       177\n",
      "     class 3       0.99      0.98      0.99      3178\n",
      "\n",
      "    accuracy                           0.98      3428\n",
      "   macro avg       0.89      0.91      0.90      3428\n",
      "weighted avg       0.98      0.98      0.98      3428\n",
      "\n",
      "Mean accuracy:  0.9752042007001167\n",
      "No of correctly classified samples:  3343\n",
      "Missed samples for each class:  9 24 52\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  87.67123287671232 %\n",
      "class 2:  86.4406779661017 %\n",
      "class 3:  98.36375078665827 %\n",
      "\n",
      "Feature selection cost:  64.11\n",
      "Fitness:  17536\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.43      0.56      0.49        73\n",
      "     class 2       0.14      0.14      0.14       177\n",
      "     class 3       0.94      0.94      0.94      3178\n",
      "\n",
      "    accuracy                           0.89      3428\n",
      "   macro avg       0.51      0.55      0.52      3428\n",
      "weighted avg       0.89      0.89      0.89      3428\n",
      "\n",
      "Mean accuracy:  0.8903150525087514\n",
      "No of correctly classified samples:  3052\n",
      "Missed samples for each class:  32 153 191\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  56.16438356164384 %\n",
      "class 2:  13.559322033898312 %\n",
      "class 3:  93.98993077407174 %\n",
      "\n",
      "Feature selection cost:  32.92\n",
      "Fitness:  749694\n",
      "######################################################################\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-31c22f79df29>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mfitness_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mpop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mpop_fitness\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_fitness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mfitness_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpop_fitness\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-efdcfb57c2f3>\u001b[0m in \u001b[0;36mevolve\u001b[1;34m(pop, clf, data, retain_percentage, random_select, mutate_prob)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mfemale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfemale_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mhalf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmale\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mchild\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmale\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mhalf\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfemale\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhalf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[0mchildren\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcceb174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
