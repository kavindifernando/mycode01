{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51327791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from random import randint, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5445b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_features(selected_features, all_features):\n",
    "    selected_f = []\n",
    "    for sample in all_features:\n",
    "        s_f = []\n",
    "        for i in range(len(selected_features)):\n",
    "            if selected_features[i] == 1:\n",
    "                s_f.append(sample[i])\n",
    "        selected_f.append(s_f)\n",
    "    return selected_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45611d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_labels(class_prob):\n",
    "    predicted_labels = []\n",
    "    \n",
    "    for p in class_prob:\n",
    "        predicted_labels.append(p.argmax() + 1)\n",
    "            \n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d782698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_miss_percentages(true_labels, predicted_labels):\n",
    "    c1_missed = 0\n",
    "    c1_tot = 0\n",
    "    c2_missed = 0\n",
    "    c2_tot = 0\n",
    "    c3_missed = 0\n",
    "    c3_tot = 0\n",
    "    \n",
    "    for i in range(len(true_labels)):\n",
    "        if true_labels[i] == 1:\n",
    "            c1_tot += 1\n",
    "            if true_labels[i] != predicted_labels[i]:\n",
    "                c1_missed += 1\n",
    "        \n",
    "        if true_labels[i] == 2:\n",
    "            c2_tot += 1\n",
    "            if true_labels[i] != predicted_labels[i]:\n",
    "                c2_missed += 1\n",
    "                \n",
    "        if true_labels[i] == 3:\n",
    "            c3_tot += 1\n",
    "            if true_labels[i] != predicted_labels[i]:\n",
    "                c3_missed += 1\n",
    "        \n",
    "    c1_miss_percent = (100.00 * c1_missed) / c1_tot  \n",
    "    c2_miss_percent = (100.00 * c2_missed) / c2_tot\n",
    "    c3_miss_percent = (100.00 * c3_missed) / c3_tot\n",
    "    \n",
    "    if c1_miss_percent <= 1:\n",
    "        c1_miss_percent = 1\n",
    "    if c2_miss_percent <= 1:\n",
    "        c2_miss_percent = 1\n",
    "    if c3_miss_percent <= 1:\n",
    "        c3_miss_percent = 1\n",
    "        \n",
    "    print( \"Missed samples for each class: \", c1_missed, c2_missed, c3_missed)\n",
    "    \n",
    "    return c1_miss_percent, c2_miss_percent, c3_miss_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e72ae9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train = open(\"ann-train.data\", \"r\")\n",
    "    training_data = []\n",
    "    for line in train:\n",
    "        training_data.append(line.strip().split(\" \"))\n",
    "    \n",
    "    convert_types(training_data)\n",
    "    \n",
    "    test = open(\"ann-test.data\", \"r\")\n",
    "    test_data = []\n",
    "    for line in test:\n",
    "        test_data.append(line.strip().split(\" \"))\n",
    "    \n",
    "    convert_types(test_data)\n",
    "    \n",
    "    training_features = []\n",
    "    training_labels = []\n",
    "    for sample in training_data:\n",
    "        training_features.append(sample[:-1])\n",
    "        training_labels.append(sample[-1])\n",
    "        \n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "    for sample in test_data:\n",
    "        test_features.append(sample[:-1])\n",
    "        test_labels.append(sample[-1])\n",
    "        \n",
    "    cost = open(\"ann-thyroid.cost\", \"r\")\n",
    "    costs = []\n",
    "    for line in cost:\n",
    "        costs.append(float(line.strip().split(\":\")[1]))\n",
    "    costs.append(0) # 21st feature is a comb. of 19th and 20th features\n",
    "        \n",
    "    return training_features, training_labels, test_features, test_labels, costs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5067d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_types(data):\n",
    "    for row in data:\n",
    "        row[0] = float(row[0].strip())\n",
    "        row[1] = int(row[1].strip())\n",
    "        row[2] = int(row[2].strip())\n",
    "        row[3] = int(row[3].strip())\n",
    "        row[4] = int(row[4].strip())\n",
    "        row[5] = int(row[5].strip())\n",
    "        row[6] = int(row[6].strip())\n",
    "        row[7] = int(row[7].strip())\n",
    "        row[8] = int(row[8].strip())\n",
    "        row[9] = int(row[9].strip())\n",
    "        row[10] = int(row[10].strip())\n",
    "        row[11] = int(row[11].strip())\n",
    "        row[12] = int(row[12].strip())\n",
    "        row[13] = int(row[13].strip())\n",
    "        row[14] = int(row[14].strip())\n",
    "        row[15] = int(row[15].strip())\n",
    "        row[16] = float(row[16].strip())\n",
    "        row[17] = float(row[17].strip())\n",
    "        row[18] = float(row[18].strip())\n",
    "        row[19] = float(row[19].strip())\n",
    "        row[20] = float(row[20].strip())\n",
    "        row[21] = int(row[21].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd6afb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual(length):\n",
    "    # create an individual, which is binary repr. of selected features\n",
    "    selected_features = [0] * len(length)\n",
    "    \n",
    "    for i in range(len(selected_features)):\n",
    "        selected_features[i] = randint(0, 1)\n",
    "        \n",
    "    # 21st feature is a comb. of 19th and 20th features\n",
    "    if selected_features[20] == 1: \n",
    "        selected_features[18] = 1\n",
    "        selected_features[19] = 1\n",
    "        \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4341936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def population(count, length):\n",
    "    # create 'count' number of individuals\n",
    "    return [individual(length) for _ in range(count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22768d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(individual, clf, data):\n",
    "    # determine the fitness of an individual\n",
    "    # misclassified class 1 (%) * misclassified class 2 (%) * misclassified class 3 (%) * feature selection cost (resulting value is converted to int)\n",
    "    # lower is better\n",
    "    print (\"\\n######################################################################\")\n",
    "    print (\"Individual: \", individual)\n",
    "    fs_cost = feature_selection_cost(individual, data[4])\n",
    "    \n",
    "    selected_train_f = get_selected_features(individual, data[0])\n",
    "    selected_test_f = get_selected_features(individual, data[2])\n",
    "\n",
    "    clf = clf.fit(selected_train_f, data[1])\n",
    "    \n",
    "    class_prob = clf.predict_proba(selected_test_f)\n",
    "    predicted_labels = get_predicted_labels(class_prob)\n",
    "    target_names = ['class 1', 'class 2', 'class 3']\n",
    "    \n",
    "    print(classification_report(data[3], predicted_labels, target_names=target_names))\n",
    "    print( \"Mean accuracy: \", clf.score(selected_test_f, data[3]))\n",
    "    print (\"No of correctly classified samples: \", accuracy_score(data[3], predicted_labels, normalize=False))\n",
    "    \n",
    "    c1_miss_percent, c2_miss_percent, c3_miss_percent = get_class_miss_percentages(data[3], predicted_labels)\n",
    "    f_result = int(c1_miss_percent * c2_miss_percent * c3_miss_percent * fs_cost)\n",
    "    \n",
    "    print (\"\\nClass accuracies: \\n\", \"class 1: \", (100 - c1_miss_percent), \"%\\nclass 2: \", (100 - c2_miss_percent), \"%\\nclass 3: \", (100 - c3_miss_percent), \"%\\n\")\n",
    "    print (\"Feature selection cost: \", fs_cost)\n",
    "    print (\"Fitness: \", f_result)\n",
    "    print (\"######################################################################\\n\")\n",
    "    \n",
    "    return f_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca108278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_fitness(pop, clf, data):\n",
    "    # average fitness of a population\n",
    "    tot_fitness = 0\n",
    "    for i in pop:\n",
    "        tot_fitness += fitness(i, clf, data)\n",
    "    return tot_fitness / len(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51967f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve(pop, clf, data, retain_percentage=0.50, random_select=0.05, mutate_prob=0.01):\n",
    "    f_values = [(fitness(i, clf, data), i) for i in pop]\n",
    "    individuals = [i[1] for i in sorted(f_values)]\n",
    "    retain_length = int(len(pop) * retain_percentage)\n",
    "    parents = individuals[:retain_length]\n",
    "    \n",
    "    # randomly add other individuals to increase diversity\n",
    "    for i in individuals[retain_length:]:\n",
    "        if random_select > random():\n",
    "            parents.append(i)\n",
    "            \n",
    "    # mutate\n",
    "    for i in parents:\n",
    "        if mutate_prob > random():\n",
    "            index_to_mutate = randint(0, len(i) - 1)\n",
    "            i[index_to_mutate] = randint(0, 1)\n",
    "            \n",
    "            # make sure the result is still valid\n",
    "            if i[20] == 1: \n",
    "                i[18] = 1\n",
    "                i[19] = 1\n",
    "    \n",
    "    # crossover\n",
    "    no_of_parents = len(parents)\n",
    "    remaining_no_of_ind = len(pop) - no_of_parents\n",
    "    children = []\n",
    "    \n",
    "    while len(children) < remaining_no_of_ind:\n",
    "        male_index = randint(0, no_of_parents - 1)\n",
    "        female_index = randint(0, no_of_parents - 1)\n",
    "        \n",
    "        if male_index != female_index:\n",
    "            male = parents[male_index]\n",
    "            female = parents[female_index]\n",
    "            half = len(male) / 2\n",
    "            child = male[:half] + female[half:]\n",
    "            children.append(child)\n",
    "    \n",
    "    parents.extend(children)\n",
    "    return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7769187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_cost(selected_features, costs):\n",
    "    total_cost = 0\n",
    "    \n",
    "    for i in range(len(selected_features)):\n",
    "        if selected_features[i] == 1:\n",
    "            total_cost += costs[i]\n",
    "            \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "615e5804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fittest_results(clf, training_features, training_labels, test_features, test_labels, costs):\n",
    "    # obtained from genetic algorithm runs\n",
    "    fittest = [0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1]\n",
    "    \n",
    "    print (\"Test accuracies\")\n",
    "    test_acc_data = [training_features, training_labels, test_features, test_labels, costs]\n",
    "    fitness(fittest, clf, test_acc_data)\n",
    "    \n",
    "    print (\"Training accuracies\")\n",
    "    train_acc_data = [training_features, training_labels, training_features, training_labels, costs]\n",
    "    fitness(fittest, clf, train_acc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4935324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    training_features, training_labels, test_features, test_labels, costs = load_data()\n",
    "    data = [training_features, training_labels, test_features, test_labels, costs]\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    \n",
    "    pop = population(20, training_features[0])\n",
    "    \n",
    "    fitness_history = []\n",
    "    for i in range(100):\n",
    "        pop = evolve(pop, clf, data)\n",
    "        pop_fitness = avg_fitness(pop, clf, data)\n",
    "        fitness_history.append(pop_fitness)\n",
    "        \n",
    "        if pop_fitness < 250:\n",
    "            break\n",
    "\n",
    "    print (\"Avg fitness history: \", fitness_history)\n",
    "    \n",
    "    # fittest_results(clf, training_features, training_labels, test_features, test_labels, costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94a3faa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "Individual:  [1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.54      0.71      0.62        73\n",
      "     class 2       0.07      0.08      0.08       177\n",
      "     class 3       0.94      0.93      0.94      3178\n",
      "\n",
      "    accuracy                           0.88      3428\n",
      "   macro avg       0.52      0.57      0.54      3428\n",
      "weighted avg       0.89      0.88      0.88      3428\n",
      "\n",
      "Mean accuracy:  0.8815635939323221\n",
      "No of correctly classified samples:  3022\n",
      "Missed samples for each class:  21 163 222\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  71.23287671232876 %\n",
      "class 2:  7.909604519774007 %\n",
      "class 3:  93.01447451227187 %\n",
      "\n",
      "Feature selection cost:  34.92\n",
      "Fitness:  646225\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.04      0.05      0.05        73\n",
      "     class 2       0.04      0.05      0.04       177\n",
      "     class 3       0.93      0.91      0.92      3178\n",
      "\n",
      "    accuracy                           0.84      3428\n",
      "   macro avg       0.33      0.34      0.33      3428\n",
      "weighted avg       0.86      0.84      0.85      3428\n",
      "\n",
      "Mean accuracy:  0.8430571761960327\n",
      "No of correctly classified samples:  2890\n",
      "Missed samples for each class:  69 169 300\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  5.479452054794521 %\n",
      "class 2:  4.5197740112994325 %\n",
      "class 3:  90.56010069225928 %\n",
      "\n",
      "Feature selection cost:  18.41\n",
      "Fitness:  1568414\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.92      0.99      0.95        73\n",
      "     class 2       0.79      0.82      0.81       177\n",
      "     class 3       0.99      0.99      0.99      3178\n",
      "\n",
      "    accuracy                           0.98      3428\n",
      "   macro avg       0.90      0.93      0.92      3428\n",
      "weighted avg       0.98      0.98      0.98      3428\n",
      "\n",
      "Mean accuracy:  0.9775379229871646\n",
      "No of correctly classified samples:  3351\n",
      "Missed samples for each class:  1 32 44\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  98.63013698630137 %\n",
      "class 2:  81.92090395480226 %\n",
      "class 3:  98.61548143486469 %\n",
      "\n",
      "Feature selection cost:  70.11\n",
      "Fitness:  2403\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.83      0.96      0.89        73\n",
      "     class 2       0.71      0.66      0.68       177\n",
      "     class 3       0.98      0.98      0.98      3178\n",
      "\n",
      "    accuracy                           0.96      3428\n",
      "   macro avg       0.84      0.87      0.85      3428\n",
      "weighted avg       0.96      0.96      0.96      3428\n",
      "\n",
      "Mean accuracy:  0.9641190198366394\n",
      "No of correctly classified samples:  3305\n",
      "Missed samples for each class:  3 61 59\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  95.89041095890411 %\n",
      "class 2:  65.5367231638418 %\n",
      "class 3:  98.14348646947766 %\n",
      "\n",
      "Feature selection cost:  58.7\n",
      "Fitness:  15434\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.66      0.77      0.71        73\n",
      "     class 2       0.70      0.78      0.74       177\n",
      "     class 3       0.99      0.98      0.98      3178\n",
      "\n",
      "    accuracy                           0.96      3428\n",
      "   macro avg       0.78      0.84      0.81      3428\n",
      "weighted avg       0.97      0.96      0.96      3428\n",
      "\n",
      "Mean accuracy:  0.9635355892648775\n",
      "No of correctly classified samples:  3303\n",
      "Missed samples for each class:  17 39 69\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  76.7123287671233 %\n",
      "class 2:  77.96610169491525 %\n",
      "class 3:  97.82882315921964 %\n",
      "\n",
      "Feature selection cost:  38.19\n",
      "Fitness:  42546\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.60      0.64      0.62        73\n",
      "     class 2       0.68      0.73      0.70       177\n",
      "     class 3       0.99      0.98      0.98      3178\n",
      "\n",
      "    accuracy                           0.96      3428\n",
      "   macro avg       0.76      0.78      0.77      3428\n",
      "weighted avg       0.96      0.96      0.96      3428\n",
      "\n",
      "Mean accuracy:  0.9612018669778296\n",
      "No of correctly classified samples:  3295\n",
      "Missed samples for each class:  26 48 59\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  64.38356164383562 %\n",
      "class 2:  72.88135593220339 %\n",
      "class 3:  98.14348646947766 %\n",
      "\n",
      "Feature selection cost:  44.19\n",
      "Fitness:  79239\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.77      0.84      0.80        73\n",
      "     class 2       0.77      0.81      0.79       177\n",
      "     class 3       0.99      0.98      0.99      3178\n",
      "\n",
      "    accuracy                           0.97      3428\n",
      "   macro avg       0.84      0.88      0.86      3428\n",
      "weighted avg       0.97      0.97      0.97      3428\n",
      "\n",
      "Mean accuracy:  0.9725787631271878\n",
      "No of correctly classified samples:  3334\n",
      "Missed samples for each class:  12 34 48\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  83.56164383561644 %\n",
      "class 2:  80.7909604519774 %\n",
      "class 3:  98.48961611076149 %\n",
      "\n",
      "Feature selection cost:  58.699999999999996\n",
      "Fitness:  27995\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.60      0.78      0.68        73\n",
      "     class 2       0.10      0.12      0.11       177\n",
      "     class 3       0.95      0.93      0.94      3178\n",
      "\n",
      "    accuracy                           0.88      3428\n",
      "   macro avg       0.55      0.61      0.57      3428\n",
      "weighted avg       0.89      0.88      0.89      3428\n",
      "\n",
      "Mean accuracy:  0.8838973162193698\n",
      "No of correctly classified samples:  3030\n",
      "Missed samples for each class:  16 156 226\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  78.08219178082192 %\n",
      "class 2:  11.86440677966101 %\n",
      "class 3:  92.88860918816866 %\n",
      "\n",
      "Feature selection cost:  32.92\n",
      "Fitness:  452233\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.53      0.79      0.63        73\n",
      "     class 2       0.10      0.12      0.11       177\n",
      "     class 3       0.94      0.92      0.93      3178\n",
      "\n",
      "    accuracy                           0.88      3428\n",
      "   macro avg       0.52      0.61      0.56      3428\n",
      "weighted avg       0.89      0.88      0.88      3428\n",
      "\n",
      "Mean accuracy:  0.8780630105017503\n",
      "No of correctly classified samples:  3010\n",
      "Missed samples for each class:  15 156 247\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  79.45205479452055 %\n",
      "class 2:  11.86440677966101 %\n",
      "class 3:  92.22781623662681 %\n",
      "\n",
      "Feature selection cost:  38.92\n",
      "Fitness:  547817\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.54      0.52      0.53        73\n",
      "     class 2       0.72      0.83      0.77       177\n",
      "     class 3       0.99      0.99      0.99      3178\n",
      "\n",
      "    accuracy                           0.97      3428\n",
      "   macro avg       0.75      0.78      0.77      3428\n",
      "weighted avg       0.97      0.97      0.97      3428\n",
      "\n",
      "Mean accuracy:  0.969369894982497\n",
      "No of correctly classified samples:  3323\n",
      "Missed samples for each class:  35 30 40\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  52.054794520547944 %\n",
      "class 2:  83.05084745762711 %\n",
      "class 3:  98.74134675896791 %\n",
      "\n",
      "Feature selection cost:  46.19\n",
      "Fitness:  47244\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.62      0.78      0.69        73\n",
      "     class 2       0.12      0.12      0.12       177\n",
      "     class 3       0.95      0.94      0.94      3178\n",
      "\n",
      "    accuracy                           0.89      3428\n",
      "   macro avg       0.56      0.62      0.59      3428\n",
      "weighted avg       0.90      0.89      0.90      3428\n",
      "\n",
      "Mean accuracy:  0.8949824970828472\n",
      "No of correctly classified samples:  3068\n",
      "Missed samples for each class:  16 155 189\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  78.08219178082192 %\n",
      "class 2:  12.42937853107344 %\n",
      "class 3:  94.05286343612335 %\n",
      "\n",
      "Feature selection cost:  47.33\n",
      "Fitness:  540256\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.67      0.73      0.70        73\n",
      "     class 2       0.81      0.88      0.84       177\n",
      "     class 3       1.00      0.99      0.99      3178\n",
      "\n",
      "    accuracy                           0.98      3428\n",
      "   macro avg       0.83      0.87      0.84      3428\n",
      "weighted avg       0.98      0.98      0.98      3428\n",
      "\n",
      "Mean accuracy:  0.9778296382730455\n",
      "No of correctly classified samples:  3352\n",
      "Missed samples for each class:  20 21 35\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  72.6027397260274 %\n",
      "class 2:  88.13559322033899 %\n",
      "class 3:  98.89867841409692 %\n",
      "\n",
      "Feature selection cost:  54.599999999999994\n",
      "Fitness:  19546\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.27      0.44      0.33        73\n",
      "     class 2       0.07      0.08      0.08       177\n",
      "     class 3       0.94      0.92      0.93      3178\n",
      "\n",
      "    accuracy                           0.86      3428\n",
      "   macro avg       0.43      0.48      0.45      3428\n",
      "weighted avg       0.88      0.86      0.87      3428\n",
      "\n",
      "Mean accuracy:  0.8623103850641773\n",
      "No of correctly classified samples:  2956\n",
      "Missed samples for each class:  41 162 269\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  43.83561643835616 %\n",
      "class 2:  8.474576271186436 %\n",
      "class 3:  91.53555695405916 %\n",
      "\n",
      "Feature selection cost:  29.82\n",
      "Fitness:  1297504\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.63      0.74      0.68        73\n",
      "     class 2       0.77      0.77      0.77       177\n",
      "     class 3       0.99      0.99      0.99      3178\n",
      "\n",
      "    accuracy                           0.97      3428\n",
      "   macro avg       0.80      0.83      0.81      3428\n",
      "weighted avg       0.97      0.97      0.97      3428\n",
      "\n",
      "Mean accuracy:  0.9719953325554259\n",
      "No of correctly classified samples:  3332\n",
      "Missed samples for each class:  19 40 37\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  73.97260273972603 %\n",
      "class 2:  77.40112994350282 %\n",
      "class 3:  98.83574575204531 %\n",
      "\n",
      "Feature selection cost:  29.78\n",
      "Fitness:  20393\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.63      0.70      0.66        73\n",
      "     class 2       0.78      0.84      0.81       177\n",
      "     class 3       1.00      0.99      0.99      3178\n",
      "\n",
      "    accuracy                           0.98      3428\n",
      "   macro avg       0.80      0.84      0.82      3428\n",
      "weighted avg       0.98      0.98      0.98      3428\n",
      "\n",
      "Mean accuracy:  0.9757876312718786\n",
      "No of correctly classified samples:  3345\n",
      "Missed samples for each class:  22 28 33\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  69.86301369863014 %\n",
      "class 2:  84.18079096045197 %\n",
      "class 3:  98.96161107614851 %\n",
      "\n",
      "Feature selection cost:  45.19\n",
      "Fitness:  22371\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.62      0.74      0.68        73\n",
      "     class 2       0.12      0.12      0.12       177\n",
      "     class 3       0.95      0.94      0.94      3178\n",
      "\n",
      "    accuracy                           0.90      3428\n",
      "   macro avg       0.56      0.60      0.58      3428\n",
      "weighted avg       0.90      0.90      0.90      3428\n",
      "\n",
      "Mean accuracy:  0.896441073512252\n",
      "No of correctly classified samples:  3073\n",
      "Missed samples for each class:  19 156 180\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  73.97260273972603 %\n",
      "class 2:  11.86440677966101 %\n",
      "class 3:  94.33606041535558 %\n",
      "\n",
      "Feature selection cost:  50.33\n",
      "Fitness:  653924\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kavindi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kavindi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kavindi\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.00      0.00      0.00        73\n",
      "     class 2       0.00      0.00      0.00       177\n",
      "     class 3       0.93      1.00      0.96      3178\n",
      "\n",
      "    accuracy                           0.93      3428\n",
      "   macro avg       0.31      0.33      0.32      3428\n",
      "weighted avg       0.86      0.93      0.89      3428\n",
      "\n",
      "Mean accuracy:  0.927071178529755\n",
      "No of correctly classified samples:  3178\n",
      "Missed samples for each class:  73 177 0\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  0.0 %\n",
      "class 2:  0.0 %\n",
      "class 3:  99 %\n",
      "\n",
      "Feature selection cost:  6.0\n",
      "Fitness:  60000\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.61      0.74      0.67        73\n",
      "     class 2       0.09      0.10      0.09       177\n",
      "     class 3       0.94      0.93      0.94      3178\n",
      "\n",
      "    accuracy                           0.88      3428\n",
      "   macro avg       0.55      0.59      0.57      3428\n",
      "weighted avg       0.89      0.88      0.89      3428\n",
      "\n",
      "Mean accuracy:  0.8838973162193698\n",
      "No of correctly classified samples:  3030\n",
      "Missed samples for each class:  19 159 220\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  73.97260273972603 %\n",
      "class 2:  10.169491525423723 %\n",
      "class 3:  93.07740717432347 %\n",
      "\n",
      "Feature selection cost:  27.919999999999998\n",
      "Fitness:  451896\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.58      0.77      0.66        73\n",
      "     class 2       0.12      0.14      0.13       177\n",
      "     class 3       0.95      0.93      0.94      3178\n",
      "\n",
      "    accuracy                           0.89      3428\n",
      "   macro avg       0.55      0.61      0.58      3428\n",
      "weighted avg       0.90      0.89      0.89      3428\n",
      "\n",
      "Mean accuracy:  0.8862310385064177\n",
      "No of correctly classified samples:  3038\n",
      "Missed samples for each class:  17 153 220\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  76.7123287671233 %\n",
      "class 2:  13.559322033898312 %\n",
      "class 3:  93.07740717432347 %\n",
      "\n",
      "Feature selection cost:  32.92\n",
      "Fitness:  458746\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "Individual:  [0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.61      0.75      0.67        73\n",
      "     class 2       0.12      0.10      0.11       177\n",
      "     class 3       0.94      0.95      0.94      3178\n",
      "\n",
      "    accuracy                           0.90      3428\n",
      "   macro avg       0.56      0.60      0.58      3428\n",
      "weighted avg       0.89      0.90      0.90      3428\n",
      "\n",
      "Mean accuracy:  0.8978996499416569\n",
      "No of correctly classified samples:  3078\n",
      "Missed samples for each class:  18 159 173\n",
      "\n",
      "Class accuracies: \n",
      " class 1:  75.34246575342466 %\n",
      "class 2:  10.169491525423723 %\n",
      "class 3:  94.55632473253618 %\n",
      "\n",
      "Feature selection cost:  44.33\n",
      "Fitness:  534519\n",
      "######################################################################\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-31c22f79df29>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mfitness_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mpop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mpop_fitness\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_fitness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mfitness_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpop_fitness\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-efdcfb57c2f3>\u001b[0m in \u001b[0;36mevolve\u001b[1;34m(pop, clf, data, retain_percentage, random_select, mutate_prob)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mfemale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfemale_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mhalf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmale\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mchild\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmale\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mhalf\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfemale\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhalf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[0mchildren\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee8770f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
